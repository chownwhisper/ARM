
---

# ğŸ”§ 1. Website / App Is Down (Unreachable or â€œConnection Timed Outâ€)

### ğŸ§  The Logic

When a site doesnâ€™t load, the issue can exist in **four main layers**:

1. **DNS** â€” domain doesnâ€™t point to the right IP
2. **Network** â€” the server canâ€™t be reached from the internet
3. **Service** â€” the server is up, but web service (nginx, node, etc.) is down
4. **Application** â€” backend is up, but app is erroring (e.g. database issue)

Understanding which layer is failing lets you **narrow scope fast**.

---

### ğŸ” Step-by-Step

#### Step 1: DNS Resolution

Run:

```bash
dig +short example.com
```

If no result â†’ DNS issue.
If IP appears â†’ DNS is fine.

> Why: DNS translates a domain to an IP. If this fails, nothing else matters.

Then check propagation:

```bash
dig example.com @8.8.8.8
dig example.com @1.1.1.1
```

If different IPs â†’ propagation delay or stale record.

---

#### Step 2: Network Reachability

```bash
ping -c 4 <ip>
traceroute <ip>
```

If no response â†’ network or firewall issue.

> Why: If packets canâ€™t reach the server, itâ€™s likely blocked upstream (firewall, routing, cloud security group).

If the server is reachable, move to layer 3.

---

#### Step 3: Web Server Layer

SSH into the host and check:

```bash
sudo systemctl status nginx
sudo systemctl status apache2
```

If â€œinactiveâ€ or â€œfailedâ€ â†’ restart:

```bash
sudo systemctl restart nginx
```

> Why: The web service might have crashed or failed to bind to port 80/443.

Check open ports:

```bash
sudo ss -tulpn | grep ':80\|:443'
```

If nothing is listening â†’ service issue.

---

#### Step 4: Application Health

Try internal request:

```bash
curl -I http://localhost
```

If it works locally but fails remotely â†’ firewall or proxy problem.
If it fails locally too â†’ app or code issue.

---

#### ğŸ§¾ Collect for DevOps/NOC

* `dig` results
* `ping` / `traceroute` output
* `systemctl` status
* `curl -I` output
* Server name and time

#### ğŸš¨ Escalation

* DNS failure â†’ DNS / NOC team
* Network failure â†’ NOC
* Service failure â†’ DevOps
* App-level failure (500, timeout, DB error) â†’ DevOps or developers

---

# ğŸ”’ 2. SSL / HTTPS Certificate Errors

### ğŸ§  The Logic

SSL ensures secure communication. Errors typically mean:

* The certificate is **expired**
* The **domain name doesnâ€™t match** the certificate (mismatch)
* The certificate **chain** is broken or incomplete

---

### ğŸ” Step-by-Step

#### Step 1: Browser Check

Open the site â†’ click the ğŸ”’ icon â†’ â€œView certificateâ€ â†’ check expiry and common name (CN).

#### Step 2: Terminal Check

```bash
openssl s_client -connect example.com:443 -servername example.com </dev/null 2>/dev/null | openssl x509 -noout -dates -subject
```

Shows:

```
notBefore=2024-01-01
notAfter=2025-01-01
subject=CN=example.com
```

> If expired or CN â‰  domain â†’ thatâ€™s the issue.

#### Step 3: Full Certificate Chain

```bash
openssl s_client -showcerts -connect example.com:443
```

If intermediate certs missing â†’ browsers may reject connection.

#### Step 4: Web Server Config

Check nginx/Apache SSL block:

```bash
sudo grep ssl_certificate /etc/nginx/sites-enabled/* 
```

> Sometimes points to the wrong `.crt` file.

---

#### ğŸ§¾ Collect for DevOps/NOC

* Browser screenshot
* Output of `openssl` commands
* Web server config path

#### ğŸš¨ Escalation

* Expired â†’ Renew via Letâ€™s Encrypt / CA (NOC or DevOps)
* Mismatch â†’ DevOps reconfigures cert paths
* Missing intermediate â†’ upload full chain cert

---

# ğŸŒ 3. DNS Issues (Domain Not Resolving or Old IP)

### ğŸ§  The Logic

DNS is like a phonebook: your browser asks, â€œWhat IP is google.com?â€
If the record hasnâ€™t updated, youâ€™re calling the wrong number.

Common causes:

* Record not published on authoritative servers
* TTL delay
* DNS cache not cleared

---

### ğŸ” Step-by-Step

#### Step 1: Check Current Record

```bash
dig example.com +noall +answer
```

Shows TTL and IP.

#### Step 2: Check Authoritative Servers

```bash
dig example.com +trace
```

Walks through the full resolution path â†’ reveals which nameserver has the old value.

#### Step 3: Global Check

Use [DNSChecker.org](https://dnschecker.org)
If some regions show old IP â†’ propagation delay.

---

#### ğŸ§¾ Collect for DevOps/NOC

* `dig +trace` output
* DNS provider screenshot (record + TTL)
* Propagation tool screenshot

#### ğŸš¨ Escalation

* Wrong IP at DNS provider â†’ NOC
* Propagation delay â†’ monitor, inform client
* TTL too high â†’ suggest lowering for future changes

---

# ğŸ³ 4. Docker Container Wonâ€™t Start / Keeps Restarting

### ğŸ§  The Logic

Docker containers isolate applications.
If one wonâ€™t start, itâ€™s usually because:

* The **app crashed** on boot
* **Missing environment variable / dependency**
* **Port conflict**
* **Wrong CMD or entrypoint**

---

### ğŸ” Step-by-Step

#### Step 1: List Containers

```bash
docker ps -a
```

If â€œExited (1)â€ or â€œExited (137)â€ â†’ crashed.

#### Step 2: Logs

```bash
docker logs <container>
```

Look for Python/Node errors or config file missing.

#### Step 3: Inspect

```bash
docker inspect <container> --format '{{.State.ExitCode}}'
```

Exit code hints at problem:

* 0 = clean exit
* 1 = app crash
* 137 = killed (OOM)
* 126/127 = permission or command missing

#### Step 4: Manual Run

```bash
docker run -it --rm <image> /bin/bash
```

Then manually run the appâ€™s start command.

> This helps isolate if the entrypoint or environment is wrong.

---

#### ğŸ§¾ Collect for DevOps

* Container name, image tag
* `docker ps -a` output
* Last 100 lines of logs
* Exit code

#### ğŸš¨ Escalation

* Crash due to code â†’ DevOps or developer
* Missing env/config â†’ DevOps updates secrets/config
* Port in use â†’ NOC checks host services

---

# â˜¸ï¸ 5. Kubernetes Pod in CrashLoopBackOff

### ğŸ§  The Logic

In Kubernetes, each pod is like a mini container environment.
CrashLoopBackOff means: â€œThe pod started, crashed, restartedâ€¦ repeatedly.â€

Common causes:

* Application error on startup
* Bad config / missing environment variables
* Health probe failures
* Resource limits too low (OOMKilled)

---

### ğŸ” Step-by-Step

#### Step 1: Check Pod Status

```bash
kubectl get pods -n <namespace>
```

If STATUS = `CrashLoopBackOff`, continue.

#### Step 2: Describe the Pod

```bash
kubectl describe pod <pod> -n <namespace>
```

Check â€œEventsâ€ section:

* `OOMKilled` â†’ resource limit issue
* `ImagePullBackOff` â†’ registry / credentials
* `Back-off restarting failed container` â†’ app crash

#### Step 3: Check Logs

```bash
kubectl logs <pod> -n <namespace>
kubectl logs -p <pod> -n <namespace>    # previous attempt
```

If you see app-level error (e.g., â€œmissing ENV variableâ€), thatâ€™s the root cause.

#### Step 4: Check Config & Resources

```bash
kubectl get deployment <name> -n <namespace> -o yaml | grep -A10 "resources:"
```

> Resource limits too small â†’ OOMKilled
> Wrong image â†’ pull failure

---

#### ğŸ§¾ Collect for DevOps

* `kubectl describe` and `kubectl logs` output
* Deployment version (image tag)
* Timestamps and recent rollouts

#### ğŸš¨ Escalation

* CrashLoopBackOff with app logs â†’ DevOps
* OOMKilled â†’ DevOps or infra to adjust limits
* ImagePullBackOff â†’ check registry credentials

---

# ğŸ’½ 6. Server Performance Degradation (High CPU / Memory)

### ğŸ§  The Logic

High usage usually means:

* Application is stuck in a loop (CPU)
* Memory leak (container or process)
* Disk full (logs not rotating)
* Too many pods/containers on one node

---

### ğŸ” Step-by-Step

#### Step 1: Check Top Processes

```bash
top -o %CPU
```

> Find which process is consuming resources.

#### Step 2: Disk Space

```bash
df -h
du -sh /var/log/*
```

> Full disk causes many services to crash (no write space).

#### Step 3: System Logs

```bash
journalctl -xe --since "10 min ago"
```

> Look for kernel OOM messages or service restarts.

#### Step 4: Docker Stats (if containerized)

```bash
docker stats
```

#### Step 5: Kubernetes Nodes

```bash
kubectl top node
kubectl top pod -A
```

> Identify which pod is using excess resources.

---

#### ğŸ§¾ Collect for DevOps/NOC

* Screenshots of top/df
* Log snippets
* `kubectl top` output
* Time of spike

#### ğŸš¨ Escalation

* App-level leak â†’ DevOps
* Disk issue â†’ NOC to clean/rotate logs
* Node resource exhaustion â†’ NOC / infra scaling

---

# âš¡ BONUS: Incident Response Flow (How to Think During Any Problem)

1. **Identify scope:** one user? one region? whole system?
2. **Reproduce:** confirm the issue yourself (never rely only on report).
3. **Check basics:** DNS â†’ network â†’ service â†’ app â†’ database.
4. **Collect facts:** logs, commands, timestamps.
5. **Communicate clearly:**

   * â€œImpact: X users affected.â€
   * â€œCause (tentative): DNS not propagating.â€
   * â€œNext step: DevOps renewing SSL cert.â€
6. **Update clients with facts only**, no guesses.
7. **Document RCA (root cause analysis)** after resolution.

---

Would you like me to continue this and create:

* A **printable â€œIncident Playbookâ€ PDF** (with commands + logic summary), or
* An **interactive â€œlab-styleâ€ tutorial** where you can actually simulate and fix each problem (DNS fail, SSL expire, pod crash, etc.) using sample commands?

That way you can practice and be 100% confident in live situations.
